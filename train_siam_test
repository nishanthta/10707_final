from collections import defaultdict
import os
import random
import torch
import torch.nn.functional as F
import torch.nn as nn
from torch.utils.data import DataLoader
from torch.nn.utils import clip_grad_norm_
from torch.cuda.amp import autocast, GradScaler
from torchvision import transforms
from tqdm import tqdm
from models.backbone.vit import ViT
from models.head.topdown_heatmap_simple_head import TopdownHeatmapSimpleHead
from models.losses import JointsMSELoss
from torch.optim import AdamW
from torch.optim.lr_scheduler import StepLR 
from torch.optim.lr_scheduler import ReduceLROnPlateau
from PIL import Image
from itertools import combinations, product
from torch.utils.data import Dataset
import datetime
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

def seed_everything(seed_value = 42):
    random.seed(seed_value)

    np.random.seed(seed_value)
    torch.manual_seed(seed_value)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed_value)
        torch.cuda.manual_seed_all(seed_value)
        torch.backends.cudnn.deterministic = True  # ensure deterministic behavior for cuDNN
        torch.backends.cudnn.benchmark = False  # it can be beneficial to turn this off as it can introduce randomness

    print(f'Set all random seeds to {seed_value}.')


class DupletDatasetCEDAR(Dataset):
    def __init__(self, base_dir, signers, transform=None):
        """
        Initialize the dataset with the directory of images and transforms.
        base_dir: The directory that contains subdirectories of original and forgery images.
        transform: Transformations to apply to each image.
        """
        self.transform = transform
        self.signers = signers
        self.pairs, self.labels = self._create_pairs(base_dir)
        self.base_dir = base_dir

    def _create_pairs(self, base_dir):
        pairs = []
        labels = []
        # Walk through the directory
        for signer in self.signers:
            subdir = os.path.join(base_dir, signer)
            originals = [os.path.join(subdir, f) for f in os.listdir(subdir) if 'original' in f]
            forgeries = [os.path.join(subdir, f) for f in os.listdir(subdir) if 'forgeries' in f]

            original_pairs = list(combinations(originals, 2))
            for pair in original_pairs:
                pairs.append(pair)
                labels.append(0)

            forgery_pairs = list(product(originals, forgeries))
            for pair in forgery_pairs:
                pairs.append(pair)
                labels.append(1)

        return pairs, labels
    
    def __getitem__(self, index):
        img1_path, img2_path = self.pairs[index]
        img1 = Image.open(img1_path)
        img2 = Image.open(img2_path)

        if self.transform:
            img1 = self.transform(img1)
            img2 = self.transform(img2)

        label = torch.tensor([self.labels[index]], dtype=torch.float32)
        return [img1, img2], label

    def __len__(self):
        return len(self.pairs)

# Example transformation
transform = transforms.Compose([
    transforms.Resize((256, 256)),
    transforms.Grayscale(num_output_channels=1),
    transforms.ToTensor()
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])


class MLPClassifier(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(MLPClassifier, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        # print(f"Input shape to MLPClassifier: {x.shape}")
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        x = self.relu(x)
        x = self.fc3(x)
        return x

class ViTPose(nn.Module):
    def __init__(self):
        super(ViTPose, self).__init__()
        # Vision Transformer Backbone
        self.backbone = ViT(
            img_size=256, 
            patch_size=16, 
            in_chans=1, 
            embed_dim=768, 
            depth=12, 
            num_heads=12, 
            mlp_ratio=4,
            qkv_bias=True,
            drop_rate=0.1,
            attn_drop_rate=0.1,
            drop_path_rate=0.1
        )
        # Initialize the keypoint head with 'extra' options
        self.keypoint_head = TopdownHeatmapSimpleHead(
            in_channels=768,
            out_channels=17,  # Assuming 17 keypoints
            num_deconv_layers=2,
            num_deconv_filters=(256, 256),
            num_deconv_kernels=(4, 4),
            extra={'final_conv_kernel': 1}  # Setting the final conv kernel size
        )
        
    def forward(self, x):
        x = self.backbone(x)
        x = self.keypoint_head(x)
        # print(f"Output shape from ViTPose: {x.shape}")
        return x
    
class ContrastiveLoss(nn.Module):
    def __init__(self, margin=1.0):
        super(ContrastiveLoss, self).__init__()
        self.margin = margin

    def forward(self, output1, output2, label):
        # Euclidean distance between output1 and output2
        
        output1_flat = output1.view(output1.size(0), -1)
        output2_flat = output2.view(output2.size(0), -1)
        euclidean_distance = F.pairwise_distance(output1_flat, output2_flat, keepdim=True)
        # Contrastive loss
        loss = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +
                          (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))

        return loss


def train_model(model, mlp_model, train_loader, val_loader, device, patience):
    train_losses = []
    val_losses = []
    optimizer = AdamW(model.parameters(), lr=0.01, betas=(0.9, 0.999), weight_decay=0.01)
    scaler = GradScaler()
    model.train()
    mlp_model.train()

    for epoch in range(20):  # Increase or adjust epochs as necessary
        total_loss = 0
        for images, targets in tqdm(train_loader):
            images = [image.to(device) for image in images]
            targets = targets.to(device)
            optimizer.zero_grad()

            with autocast():
                output1 = model(images[0])
                output2 = model(images[1])
                output = torch.cat((output1, output2), dim=1)
                output = output.view(output.size(0), -1) 
                final_output = mlp_model(output)
                loss = nn.BCEWithLogitsLoss()(final_output, targets)

            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
            total_loss += loss.item()

        avg_train_loss = total_loss / len(train_loader)
        train_losses.append(avg_train_loss)
        print(f'Epoch {epoch+1}, Train Loss: {avg_train_loss}')

    #     if val_loader is not None:
    #         model.eval()
    #         mlp_model.eval()
    #         val_loss = 0.0
    #         with torch.no_grad():
    #             for images, targets in tqdm(val_loader):
    #                 images = [image.to(device) for image in images]
    #                 targets = targets.to(device)
    #                 output1 = model(images[0])
    #                 output2 = model(images[1])
    #                 output = torch.cat((output1, output2), dim=1)
    #                 final_output = mlp_model(output)
    #                 loss = nn.BCEWithLogitsLoss()(final_output, targets)
    #                 val_loss += loss.item()
            
    #         avg_val_loss = val_loss / len(val_loader)
    #         val_losses.append(avg_val_loss)
    #         print(f'Epoch {epoch+1}, Val Loss: {avg_val_loss}')

    # return train_losses, val_losses if val_loader else train_losses
        


def save_model(model, epoch, loss, descriptor="ViTPose"):
    timestamp = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
    filename = f"{descriptor}_Epoch{epoch}_Loss{loss:.4f}_{timestamp}.pth"
    torch.save(model.state_dict(), filename)
    print(f"Saved model as {filename}")

# def validate_model(model, mlp_model, dataloader, device, criterion):
#     model.eval()
#     mlp_model.eval()
#     val_loss = 0.0
#     with torch.no_grad():
#         for images, targets in dataloader:
#             images = [image.to(device) for image in images]
#             targets = targets.to(device)
#             output1 = model(images[0])
#             output2 = model(images[1])
#             output = torch.reshape(torch.cat((output1, output2), dim=1), (targets.size(0), -1))
#             final_output = mlp_model(output)
#             loss = criterion(final_output, targets)
#             val_loss += loss.item()
            
#         avg_val_loss = val_loss / len(dataloader)
        
#         return avg_val_loss


def main():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = ViTPose().to(device)
    mlp_model = MLPClassifier(input_dim=34*64*64, hidden_dim=512, output_dim=1).to(device)

    
    # dirs = defaultdict(list)
    dir  = '/home/jamesemi/Desktop/james/adl/ViTPose_pytorch/datasets/CEDAR/train_subset'
    # Reduce dataset size to a small subset for overfitting test
    # train_signers = [name for name in os.listdir(dirs['train_subset']) if os.path.isdir(os.path.join(dirs['train_subset'], name))]  # replace 'small_subset_folder' with the actual small subset
    # Make sure dirs['train'] is a string that points to the directory containing the training data.
    train_signers = [name for name in os.listdir(dir) if os.path.isdir(os.path.join(dir, name))]


    train_dataset = DupletDatasetCEDAR('/home/jamesemi/Desktop/james/adl/ViTPose_pytorch/datasets/CEDAR/train_subset', transform=transform, signers=train_signers)
    train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)  # Small batch size for clear overfitting

    # Only use train_loader for simplification, remove val_loader and test_loader
    train_losses = train_model(model, mlp_model, train_loader, None, device, patience=10)
    
    # Plotting is the same, just simpler without validation loss
    plt.figure(figsize=(10, 5))
    sns.lineplot(x=range(1, len(train_losses) + 1), y=train_losses, label='Training Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.title('Training Loss Overfitting Check')
    plt.legend()
    plt.grid(True)
    plt.show()

if __name__ == '__main__':
    main()
    
